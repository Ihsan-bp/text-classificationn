{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "552f7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt #importing matplotlib\n",
    "%matplotlib inline\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fad7b725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Classification\n",
       "0                            i didnt feel humiliated        sadness\n",
       "1  i can go from feeling so hopeless to so damned...        sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong          anger\n",
       "3  i am ever feeling nostalgic about the fireplac...           love\n",
       "4                               i am feeling grouchy          anger"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/IHSAN B P/Desktop/projects/text classification/Data for AI Assignment - Sheet1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da3c71f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am ever feeling nostalgic about the fireplace i will know that it is still on the property'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Text[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7fe8630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18001, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16c53852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed records:\n",
      "i feel on the verge of tears from weariness i look at your sweet face and cant help but tenderly kiss your cheeks\n",
      "i still feel a craving for sweet food\n",
      "i tend to stop breathing when i m feeling stressed\n",
      "i was intensely conscious of how much cash i had left in my gas and food envelope and i still have what i intended to save for next week which helps me not feel so stressed and scared\n",
      "im still not sure why reilly feels the need to be so weird\n",
      "i am not amazing or great at photography but i feel passionate about it\n",
      "ive also made it with both sugar measurements but i feel like cup is just too sweet for me\n",
      "i had to choose the sleek and smoother feel of the sweet revenge made drawing and handling the blaster a bit nicer\n",
      "i often find myself feeling assaulted by a multitude of sense impressions\n",
      "i feel im being generous with that statement\n",
      "i feel pretty tortured because i work a job and often the inspiration strikes while im at work\n",
      "i feel most passionate about\n",
      "i was so stubborn and that it took you getting hurt for me to admit even to myself how i feel i haven t been very considerate of you in that respect\n",
      "i write these words i feel sweet baby kicks from within and my memory is refreshed i would do anything for this boy\n",
      "i feel a remembrance of the strange by justin aryiku falls into the latter category\n",
      "i have chose for myself that makes me feel amazing\n",
      "i still feel completely accepted\n",
      "i feel so weird about it\n",
      "i cant escape the tears of sadness and just true grief i feel at the loss of my sweet friend and sister\n",
      "i feel like a tortured artist when i talk to her\n",
      "i feel more adventurous willing to take risks img src http cdn\n",
      "i feel like i am very passionate about youtube and so id quite like to explain why i think youtube is the next best thing for entertainment\n",
      "i feel kind of strange\n",
      "i could feel myself hit this strange foggy wall\n",
      "i feel pretty weird blogging about deodorant but im a bit of a deodorant snob and find it really hard to find a good one\n",
      "i resorted to yesterday the post peak day of illness when i was still housebound but feeling agitated and peckish for brew a href http pics\n",
      "i will feel as though i am accepted by as well as comfortable being around both sides of my family\n",
      "i shy away from songs that talk about how i feel toward god or that maybe even talk about my faithful response toward god\n",
      "i bet taylor swift basks in the knowledge that the boys she writes songs about probably feel tortured\n",
      "i began to feel accepted by gaia on her own terms\n",
      "i was sitting in the corner stewing in my own muck feeling hated alone unworthy and violated\n",
      "i feel like this was such a rude comment\n",
      "i realized what i am passionate about helping women feel accepted and appreciated\n",
      "i feel so blessed and honored that we get to be its parents\n",
      "i could feel his breath on me and smell the sweet scent of him\n",
      "i loved the feeling i got during an amazing slalom run whether it was in training or in a race\n",
      "i am feeling stressed and more than a bit anxious\n",
      "i found myself feeling inhibited and shushing her quite a lot\n",
      "i feel the need to pimp this since raini my beloved rocky casting director loves it so much\n",
      "i feel cared for and accepted\n",
      "i have not conducted a survey but it is quite likely that many of them feel as assaulted by onel s demons and other creators as i would have felt had the walls been covered only with eminent figures patriotic heroes and epic deeds\n",
      "i feel so weird and scattered with all wonders about a million different things\n",
      "i feel like some of you have pains and you cannot imagine becoming passionate about the group or the idea that is causing pain\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates and list removed records\n",
    "duplicates = df[df.duplicated(subset='Text', keep='first')]\n",
    "df = df.drop_duplicates(subset='Text', keep='first')\n",
    "\n",
    "# List removed records\n",
    "removed_records = duplicates['Text'].tolist()\n",
    "print(\"Removed records:\")\n",
    "for record in removed_records:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8454e62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17958, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "080269d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of profanities to filter\n",
    "profanities = ['1 man 1 jar', '1m1j', '1man1jar', '2 girls 1 cup', '2g1c', '2girls1cup', 'acrotomophile', 'acrotomophilia', 'alabama hot pocket', 'alabama tuna melt', 'alaskan pipeline', 'algophile', 'algophilia', 'anal', 'anal assassin', 'anal astronaut', 'anilingus', 'anus', 'ape shit', 'ape-shit', 'apeshit', 'apotemnophile', 'apotemnophilia', 'arse', 'arse bandit', 'arsehole', 'ass', 'ass bandit', 'asshole', 'auto erotic', 'autoerotic', 'babeland', 'baby batter', 'baby gravy', 'baby juice', 'ball batter', 'ball gag', 'ball gravy', 'ball kicking', 'ball licking', 'ball sack', 'ball sucking', 'ball-gag', 'ball-kicking', 'ball-licking', 'ball-sucking', 'ballcuzi', 'ballgag', 'bang bros', 'bang bus', 'bangbros', 'bangbus', 'bareback', 'barely legal', 'bastard', 'bastinado', 'batty boi', 'batty boy', 'battyboi', 'battyboy', 'bdsm', 'bean flicker', 'bean queen', 'bean-flicker', 'beaner', 'beaners', 'beanflicker', 'beastiality', 'beaver cleaver', 'beaver lips', 'beestiality', 'bellend', 'bellesa', 'bestiality', 'bicon', 'big boobs', 'big breasts', 'big cock', 'big knockers', 'big tits', 'birdlock', 'bitch', 'bitches', 'black cock', 'bloody', 'blow job', 'blow your load', 'blow-job', 'blowjob', 'blue waffle', 'bluewaffle', 'blumpkin', 'bollocks', 'bone smuggler', 'bone-smuggler', 'boner', 'bonesmuggler', 'boob', 'booty buffer', 'booty call', 'booty-buffer', 'boston george', 'breasts', 'brown piper', 'brown shower', 'brown showers', 'brown-piper', 'brownie king', 'brownie queen', 'brownpiper', 'buddha head', 'buddha-head', 'buddhahead', 'bufter', 'bufty', 'bugger', 'bukkake', 'bull shit', 'bull-shit', 'bulldyke', 'bullet vibe', 'bullet vibrator', 'bullshit', 'bum boy', 'bum chum', 'bum driller', 'bum pilot', 'bum pirate', 'bum rider', 'bum robber', 'bum rustler', 'bum-boy', 'bum-chum', 'bum-driller', 'bum-pirate', 'bum-robber', 'bumboy', 'bumchum', 'bumdriller', 'bumhole engineer', 'bumrider', 'bumrobber', 'butt boy', 'butt pilot', 'butt pirate', 'butt rider', 'butt robber', 'butt rustler', 'butt-boy', 'butt-pirate', 'butt-robber', 'buttboy', 'butthole engineer', 'buttrider', 'buttrobber', 'camel jockey', 'camel jockies', 'camel toe', 'cameljockey', 'cameljockies', 'canadian porch swing', 'carpet muncher', 'carpetmuncher', 'cheese eating surrender monkey', 'cheese-eating surrender monkey', 'chi chi man', 'chi-chi man', 'chicken queen', 'china man', 'china men', 'chinaman', 'chinamen', 'ching chong', 'ching-chong', 'chink', 'chinks', 'chinky', 'chocolate rosebud', 'chocolate rosebuds', 'cholerophile', 'cholerophilia', 'christ', 'cialis', 'circle-jerk', 'circlejerk', 'cishet', 'cissie', 'cissy', 'claustrophile', 'claustrophilia', 'cleveland accordion', 'cleveland hot waffle', 'cleveland steamer', 'clit', 'clitoris', 'clover clamp', 'clover clamps', 'clunge', 'cluster fuck', 'cluster-fuck', 'clusterfuck', 'cock', 'cockpipe cosmonaut', 'cockstruction worker', 'coimetrophile', 'coimetrophilia', 'collared', 'collaring', 'coon', 'coons', 'coprolagnia', 'coprophile', 'coprophilia', 'cornhole', 'crafty butcher', 'crap', 'cream-pie', 'creampie', 'cum', 'cum shot', 'cum shots', 'cumming', 'cumshot', 'cumshots', 'cunnilingus', 'cunt', 'cunt boy', 'cunt-boy', 'cuntboy', 'cunts', 'curry muncher', 'curry-muncher', 'currymuncher', 'damn', 'darkey', 'darkie', 'darkies', 'darky', 'date rape', 'daterape', 'ddlg', 'deep throat', 'deep-throat', 'deepthroat', 'dendrophile', 'dendrophilia', 'dick', 'dick girl', 'dick-girl', 'dickgirl', 'dildo', 'dildos', 'dingleberries', 'dingleberry', 'dipsea', 'dirty pillows', 'dirty sanchez', 'dishabiliophile', 'dishabiliophilia', 'dog shit', 'dog style', 'dog-shit', 'doggie style', 'doggie-style', 'doggiestyle', 'doggy style', 'doggy-style', 'doggystyle', 'dogshit', 'dolcett', 'domination', 'dominatrix', 'domme', 'dommes', 'donkey punch', 'donut muncher', 'donut puncher', 'doon coon', 'dooncoon', 'double penetration', 'dp action', 'dry hump', 'dune coon', 'dune-coon', 'dutch rudder', 'dyke', 'dystychiphile', 'dystychiphilia', 'edge play', 'edgeplay', 'ejaculate', 'ejaculated', 'ejaculating', 'ejaculation', 'electro-play', 'electroplay', 'emetophile', 'emetophilia', 'enby', 'eskimo trebuchet', 'eye-tie', 'eyetie', 'fag', 'fag bomb', 'fag-bomb', 'fagbomb', 'faggot', 'fagot', 'felch', 'felching', 'fellating', 'fellatio', 'female squirting', 'figging', 'finger bang', 'fingerbang', 'fingerbanging', 'fingered', 'fingering', 'finocchio', 'finoccio', 'finochio', 'fisted', 'fisting', 'foot job', 'foot-job', 'footjob', 'french rudder', 'frog eater', 'frog-eater', 'frogeater', 'frolic me', 'frolicme', 'frottage', 'frotting', 'fuck', 'fuck-wit', 'fucken', 'fucker', 'fuckers', 'fuckhead', 'fuckheads', 'fuckin', 'fucking', 'fucks', 'fucktard', 'fucktards', 'fuckwad', 'fuckwads', 'fuckwhit', 'fuckwit', 'fuckwits', 'fudge packer', 'fudge-packer', 'fudgepacker', 'futanari', 'g-spot', 'gang bang', 'gangbang', 'gay sex', 'gaysian', 'genitals', 'genitorture', 'gerontophile', 'gerontophilia', 'giant cock', 'gin jockey', 'gin jocky', 'girl on top', 'go-kun', 'goatcx', 'goatse', 'god damn', 'god damned', 'god-damn', 'god-damned', 'goddamn', 'goddamned', 'gokkun', 'golden shower', 'golden showers', 'golliwog', 'gollywog', 'gook', 'gook-eye', 'gookie', 'gooks', 'gooky', 'goregasm', 'gray queen', 'greaseball', 'grey queen', 'grope', 'group sex', 'gym bunny', 'gymbunny', 'hadji', 'haji', 'hajji', 'hand job', 'hand-job', 'handjob', 'heimie', 'hell', 'hermie', 'hickory switch', 'hippophile', 'hippophilia', 'homoerotic', 'honkey', 'honkeys', 'honkies', 'honky', 'horny', 'horse shit', 'horse-shit', 'horseshit', 'hot carl', 'hot richard', 'huge cock', 'humping', 'hymie', 'impact play', 'impact-play', 'incest', 'intercourse', 'jack off', 'jack-off', 'jail bait', 'jailbait', 'jap', 'jelly donut', 'jerk mate', 'jerk off', 'jerk-off', 'jerkmate', 'jesus', 'jesus christ', 'jigaboo', 'jiggerboo', 'jizz', 'juggs', 'jungle bunny', 'junglebunny', 'kennebunkport surprise', 'kentucky klondike', 'kentucky tractor puller', 'kike', 'kinbaku', 'kitty puncher', 'kitty-puncher', 'kittypuncher', 'knobbing', 'kraut', 'krauts', 'kunt', 'kunts', 'kynophile', 'kynophilia', 'lady boy', 'lady-boy', 'ladyboy', 'leather restraint', 'leather straight jacket', 'lemon party', 'lemonparty', 'leningrad steamer', 'lesbo', 'leso', 'lezzie', 'lezzies', 'light in the fedora', 'light in the loafers', 'light in the pants', 'limp wristed', 'limp-wristed', 'literotica', 'lovemaking', 'male squirting', 'male-squirting', 'massive cock', 'masterb8', 'masterbate', 'masturb8', 'masturbate', 'masturbating', 'masturbation', 'mayonnaise monkey', 'mayonnaise monkies', 'mdlb', 'meat masseuse', 'meat spin', 'meatspin', 'menage a trois', 'menage-a-trois', 'menages a trois', 'menages-a-trois', 'menophile', 'menophilia', 'mexican pancake', 'milwaukee blizzard', 'missionary position', 'mississippi birdbath', 'mound of venus', 'mr hands', 'mr. hands', 'mrhands', 'muff diver', 'muff diver', 'muff diving', 'muff-diver', 'muffdiver', 'muffdiver', 'muffdiving', 'muscle mary', 'mvtube', 'nambla', 'necrophile', 'necrophilia', 'negro', 'neo nazi', 'neo-nazi', 'neonazi', 'nig nog', 'nigerian hurricane', 'nigga', 'nigger', 'niggs', 'nignog', 'nimpho', 'nimphomania', 'nimphomaniac', 'nipple', 'nipple clamp', 'nipple clamps', 'nipples', 'nude', 'nudity', 'nutten', 'nympho', 'nymphomania', 'nymphomaniac', 'octopussy', 'oklahomo', 'omorashi', 'one cup two girls', 'one jar one man', 'one man one jar', 'only fans', 'onlyfans', 'orgasm', 'orgasmic', 'orgasms', 'paedo bear', 'paedobear', 'paedophile', 'paedophilia', 'pain slut', 'painslut', 'paki', 'panamanian petting zoo', 'pansy', 'panties', 'parthenophile', 'parthenophilia', 'pedo bear', 'pedobear', 'pedophile', 'pedophilia', 'pegging', 'penis', 'peter puffer', 'peter-puffer', 'peterpuffer', 'petrol sniffer', 'petrol-sniffer', 'petrolsniffer', 'phagophile', 'phagophilia', 'piece of shit', 'pieces of shit', 'pikey', 'pikeys', 'piss off', 'piss pig', 'piss pig', 'pissed off', 'pissing', 'pisspig', 'pisspig', 'playboy', 'pleasure chest', 'pnigerophile', 'pnigerophilia', 'pnigophile', 'pnigophilia', 'poinephile', 'poinephilia', 'pony boy', 'pony girl', 'pony-boy', 'pony-girl', 'pony-play', 'ponyboy', 'ponygirl', 'ponyplay', 'poof', 'poon', 'poontang', 'poop chute', 'poopchute', 'porn', 'porn hub', 'pornhub', 'porno', 'pornographic', 'pornography', 'pornos', 'potato queen', 'prince albert piercing', 'proctophile', 'proctophilia', 'pubes', 'punani', 'punany', 'pussy', 'pussy puncher', 'pussy-puncher', 'pussypuncher', 'queaf', 'queef', 'quim', 'rag head', 'rag heads', 'raghead', 'ragheads', 'raging boner', 'ramen yarmulke', 'rape', 'raping', 'rapist', 'rectum', 'retard', 'retarded', 'reverse cowgirl', 'rhabdophile', 'rhabdophilia', 'rhypophile', 'rhypophilia', 'rice queen', 'rimjob', 'rimming', 'ring raider', 'ringraider', 'rusty trombone', 'sand nigger', 'sand-nigger', 'sandnigger', 'santorum', 'scatophile', 'scatophilia', 'schlong', 'scissoring', 'semen', 'seplophile', 'seplophilia', 'sex', 'shaved beaver', 'shaved pussy', 'she male', 'she-male', 'sheep shagger', 'sheepshagger', 'shemale', 'shibari', 'shit', 'shit head', 'shithead', 'shitty', 'shlong', 'shota', 'shrimping', 'sissy', 'skeet', 'skittle harvest', 'skittles harvest', 'slant eye', 'slant-eye', 'slanteye', 'snatch', 'snowballing', 'sod off', 'sodding', 'sodomise', 'sodomist', 'sodomize', 'sodomy', 'spastic', 'spearchucker', 'spic', 'spick', 'spicks', 'spics', 'spicy gringo', 'splooge', 'splooge moose', 'spooge', 'spunk', 'strap on', 'strap-on', 'strap-on', 'strapon', 'strappado', 'suastika', 'svastika', 'swamp guinea', 'swamp-guinea', 'swastika', 'switch hitter', 't-girl', 'taphephile', 'taphephilia', 'tea bagged', 'tea bagging', 'tea-bagged', 'tea-bagging', 'tgirl', 'thanatophile', 'thanatophilia', 'threesome', 'throating', 'throbbing boner', 'throbbing cock', 'thumbzilla', 'timber nigger', 'timber-nigger', 'timbernigger', 'tits', 'titties', 'titty', 'topless', 'tosser', 'towel head', 'towel-head', 'towelhead', 'trannie', 'tranny', 'transbian', 'traumatophile', 'traumatophilia', 'tribadism', 'tribbing', 'tub girl', 'tubgirl', 'twat', 'twink', 'two girls one cup', 'urethra play', 'urophile', 'urophilia', 'vagina', 'venus mound', 'viagra', 'vibrator', 'violet wand', 'vorarephile', 'vorarephilia', 'voyeurweb', 'wagon burner', 'wagon-burner', 'wank', 'wanker', 'wax play', 'wax-play', 'wet back', 'wet dream', 'wet-back', 'wetback', 'whigger', 'white power', 'white-power', 'whitepower', 'whore', 'wigga', 'wigger', 'wiitwd', 'wog', 'wogs', 'wolfbagging', 'worldsex', 'wrapping men', 'wrinkled starfish', 'xhamster', 'xnxx', 'xtube', 'xvideos', 'xxx', 'xyrophile', 'xyrophilia', 'yellow shower', 'yellow showers', 'zipper head', 'zipper-head', 'zipperhead', 'zippo cat', 'zippo-cat', 'zippocat', 'zoophile', 'zoophilia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96c0e177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 man 1 jar',\n",
       " '1m1j',\n",
       " '1man1jar',\n",
       " '2 girls 1 cup',\n",
       " '2g1c',\n",
       " '2girls1cup',\n",
       " 'acrotomophile',\n",
       " 'acrotomophilia',\n",
       " 'alabama hot pocket',\n",
       " 'alabama tuna melt',\n",
       " 'alaskan pipeline',\n",
       " 'algophile',\n",
       " 'algophilia',\n",
       " 'anal',\n",
       " 'anal assassin',\n",
       " 'anal astronaut',\n",
       " 'anilingus',\n",
       " 'anus',\n",
       " 'ape shit',\n",
       " 'ape-shit',\n",
       " 'apeshit',\n",
       " 'apotemnophile',\n",
       " 'apotemnophilia',\n",
       " 'arse',\n",
       " 'arse bandit',\n",
       " 'arsehole',\n",
       " 'ass',\n",
       " 'ass bandit',\n",
       " 'asshole',\n",
       " 'auto erotic',\n",
       " 'autoerotic',\n",
       " 'babeland',\n",
       " 'baby batter',\n",
       " 'baby gravy',\n",
       " 'baby juice',\n",
       " 'ball batter',\n",
       " 'ball gag',\n",
       " 'ball gravy',\n",
       " 'ball kicking',\n",
       " 'ball licking',\n",
       " 'ball sack',\n",
       " 'ball sucking',\n",
       " 'ball-gag',\n",
       " 'ball-kicking',\n",
       " 'ball-licking',\n",
       " 'ball-sucking',\n",
       " 'ballcuzi',\n",
       " 'ballgag',\n",
       " 'bang bros',\n",
       " 'bang bus',\n",
       " 'bangbros',\n",
       " 'bangbus',\n",
       " 'bareback',\n",
       " 'barely legal',\n",
       " 'bastard',\n",
       " 'bastinado',\n",
       " 'batty boi',\n",
       " 'batty boy',\n",
       " 'battyboi',\n",
       " 'battyboy',\n",
       " 'bdsm',\n",
       " 'bean flicker',\n",
       " 'bean queen',\n",
       " 'bean-flicker',\n",
       " 'beaner',\n",
       " 'beaners',\n",
       " 'beanflicker',\n",
       " 'beastiality',\n",
       " 'beaver cleaver',\n",
       " 'beaver lips',\n",
       " 'beestiality',\n",
       " 'bellend',\n",
       " 'bellesa',\n",
       " 'bestiality',\n",
       " 'bicon',\n",
       " 'big boobs',\n",
       " 'big breasts',\n",
       " 'big cock',\n",
       " 'big knockers',\n",
       " 'big tits',\n",
       " 'birdlock',\n",
       " 'bitch',\n",
       " 'bitches',\n",
       " 'black cock',\n",
       " 'bloody',\n",
       " 'blow job',\n",
       " 'blow your load',\n",
       " 'blow-job',\n",
       " 'blowjob',\n",
       " 'blue waffle',\n",
       " 'bluewaffle',\n",
       " 'blumpkin',\n",
       " 'bollocks',\n",
       " 'bone smuggler',\n",
       " 'bone-smuggler',\n",
       " 'boner',\n",
       " 'bonesmuggler',\n",
       " 'boob',\n",
       " 'booty buffer',\n",
       " 'booty call',\n",
       " 'booty-buffer',\n",
       " 'boston george',\n",
       " 'breasts',\n",
       " 'brown piper',\n",
       " 'brown shower',\n",
       " 'brown showers',\n",
       " 'brown-piper',\n",
       " 'brownie king',\n",
       " 'brownie queen',\n",
       " 'brownpiper',\n",
       " 'buddha head',\n",
       " 'buddha-head',\n",
       " 'buddhahead',\n",
       " 'bufter',\n",
       " 'bufty',\n",
       " 'bugger',\n",
       " 'bukkake',\n",
       " 'bull shit',\n",
       " 'bull-shit',\n",
       " 'bulldyke',\n",
       " 'bullet vibe',\n",
       " 'bullet vibrator',\n",
       " 'bullshit',\n",
       " 'bum boy',\n",
       " 'bum chum',\n",
       " 'bum driller',\n",
       " 'bum pilot',\n",
       " 'bum pirate',\n",
       " 'bum rider',\n",
       " 'bum robber',\n",
       " 'bum rustler',\n",
       " 'bum-boy',\n",
       " 'bum-chum',\n",
       " 'bum-driller',\n",
       " 'bum-pirate',\n",
       " 'bum-robber',\n",
       " 'bumboy',\n",
       " 'bumchum',\n",
       " 'bumdriller',\n",
       " 'bumhole engineer',\n",
       " 'bumrider',\n",
       " 'bumrobber',\n",
       " 'butt boy',\n",
       " 'butt pilot',\n",
       " 'butt pirate',\n",
       " 'butt rider',\n",
       " 'butt robber',\n",
       " 'butt rustler',\n",
       " 'butt-boy',\n",
       " 'butt-pirate',\n",
       " 'butt-robber',\n",
       " 'buttboy',\n",
       " 'butthole engineer',\n",
       " 'buttrider',\n",
       " 'buttrobber',\n",
       " 'camel jockey',\n",
       " 'camel jockies',\n",
       " 'camel toe',\n",
       " 'cameljockey',\n",
       " 'cameljockies',\n",
       " 'canadian porch swing',\n",
       " 'carpet muncher',\n",
       " 'carpetmuncher',\n",
       " 'cheese eating surrender monkey',\n",
       " 'cheese-eating surrender monkey',\n",
       " 'chi chi man',\n",
       " 'chi-chi man',\n",
       " 'chicken queen',\n",
       " 'china man',\n",
       " 'china men',\n",
       " 'chinaman',\n",
       " 'chinamen',\n",
       " 'ching chong',\n",
       " 'ching-chong',\n",
       " 'chink',\n",
       " 'chinks',\n",
       " 'chinky',\n",
       " 'chocolate rosebud',\n",
       " 'chocolate rosebuds',\n",
       " 'cholerophile',\n",
       " 'cholerophilia',\n",
       " 'christ',\n",
       " 'cialis',\n",
       " 'circle-jerk',\n",
       " 'circlejerk',\n",
       " 'cishet',\n",
       " 'cissie',\n",
       " 'cissy',\n",
       " 'claustrophile',\n",
       " 'claustrophilia',\n",
       " 'cleveland accordion',\n",
       " 'cleveland hot waffle',\n",
       " 'cleveland steamer',\n",
       " 'clit',\n",
       " 'clitoris',\n",
       " 'clover clamp',\n",
       " 'clover clamps',\n",
       " 'clunge',\n",
       " 'cluster fuck',\n",
       " 'cluster-fuck',\n",
       " 'clusterfuck',\n",
       " 'cock',\n",
       " 'cockpipe cosmonaut',\n",
       " 'cockstruction worker',\n",
       " 'coimetrophile',\n",
       " 'coimetrophilia',\n",
       " 'collared',\n",
       " 'collaring',\n",
       " 'coon',\n",
       " 'coons',\n",
       " 'coprolagnia',\n",
       " 'coprophile',\n",
       " 'coprophilia',\n",
       " 'cornhole',\n",
       " 'crafty butcher',\n",
       " 'crap',\n",
       " 'cream-pie',\n",
       " 'creampie',\n",
       " 'cum',\n",
       " 'cum shot',\n",
       " 'cum shots',\n",
       " 'cumming',\n",
       " 'cumshot',\n",
       " 'cumshots',\n",
       " 'cunnilingus',\n",
       " 'cunt',\n",
       " 'cunt boy',\n",
       " 'cunt-boy',\n",
       " 'cuntboy',\n",
       " 'cunts',\n",
       " 'curry muncher',\n",
       " 'curry-muncher',\n",
       " 'currymuncher',\n",
       " 'damn',\n",
       " 'darkey',\n",
       " 'darkie',\n",
       " 'darkies',\n",
       " 'darky',\n",
       " 'date rape',\n",
       " 'daterape',\n",
       " 'ddlg',\n",
       " 'deep throat',\n",
       " 'deep-throat',\n",
       " 'deepthroat',\n",
       " 'dendrophile',\n",
       " 'dendrophilia',\n",
       " 'dick',\n",
       " 'dick girl',\n",
       " 'dick-girl',\n",
       " 'dickgirl',\n",
       " 'dildo',\n",
       " 'dildos',\n",
       " 'dingleberries',\n",
       " 'dingleberry',\n",
       " 'dipsea',\n",
       " 'dirty pillows',\n",
       " 'dirty sanchez',\n",
       " 'dishabiliophile',\n",
       " 'dishabiliophilia',\n",
       " 'dog shit',\n",
       " 'dog style',\n",
       " 'dog-shit',\n",
       " 'doggie style',\n",
       " 'doggie-style',\n",
       " 'doggiestyle',\n",
       " 'doggy style',\n",
       " 'doggy-style',\n",
       " 'doggystyle',\n",
       " 'dogshit',\n",
       " 'dolcett',\n",
       " 'domination',\n",
       " 'dominatrix',\n",
       " 'domme',\n",
       " 'dommes',\n",
       " 'donkey punch',\n",
       " 'donut muncher',\n",
       " 'donut puncher',\n",
       " 'doon coon',\n",
       " 'dooncoon',\n",
       " 'double penetration',\n",
       " 'dp action',\n",
       " 'dry hump',\n",
       " 'dune coon',\n",
       " 'dune-coon',\n",
       " 'dutch rudder',\n",
       " 'dyke',\n",
       " 'dystychiphile',\n",
       " 'dystychiphilia',\n",
       " 'edge play',\n",
       " 'edgeplay',\n",
       " 'ejaculate',\n",
       " 'ejaculated',\n",
       " 'ejaculating',\n",
       " 'ejaculation',\n",
       " 'electro-play',\n",
       " 'electroplay',\n",
       " 'emetophile',\n",
       " 'emetophilia',\n",
       " 'enby',\n",
       " 'eskimo trebuchet',\n",
       " 'eye-tie',\n",
       " 'eyetie',\n",
       " 'fag',\n",
       " 'fag bomb',\n",
       " 'fag-bomb',\n",
       " 'fagbomb',\n",
       " 'faggot',\n",
       " 'fagot',\n",
       " 'felch',\n",
       " 'felching',\n",
       " 'fellating',\n",
       " 'fellatio',\n",
       " 'female squirting',\n",
       " 'figging',\n",
       " 'finger bang',\n",
       " 'fingerbang',\n",
       " 'fingerbanging',\n",
       " 'fingered',\n",
       " 'fingering',\n",
       " 'finocchio',\n",
       " 'finoccio',\n",
       " 'finochio',\n",
       " 'fisted',\n",
       " 'fisting',\n",
       " 'foot job',\n",
       " 'foot-job',\n",
       " 'footjob',\n",
       " 'french rudder',\n",
       " 'frog eater',\n",
       " 'frog-eater',\n",
       " 'frogeater',\n",
       " 'frolic me',\n",
       " 'frolicme',\n",
       " 'frottage',\n",
       " 'frotting',\n",
       " 'fuck',\n",
       " 'fuck-wit',\n",
       " 'fucken',\n",
       " 'fucker',\n",
       " 'fuckers',\n",
       " 'fuckhead',\n",
       " 'fuckheads',\n",
       " 'fuckin',\n",
       " 'fucking',\n",
       " 'fucks',\n",
       " 'fucktard',\n",
       " 'fucktards',\n",
       " 'fuckwad',\n",
       " 'fuckwads',\n",
       " 'fuckwhit',\n",
       " 'fuckwit',\n",
       " 'fuckwits',\n",
       " 'fudge packer',\n",
       " 'fudge-packer',\n",
       " 'fudgepacker',\n",
       " 'futanari',\n",
       " 'g-spot',\n",
       " 'gang bang',\n",
       " 'gangbang',\n",
       " 'gay sex',\n",
       " 'gaysian',\n",
       " 'genitals',\n",
       " 'genitorture',\n",
       " 'gerontophile',\n",
       " 'gerontophilia',\n",
       " 'giant cock',\n",
       " 'gin jockey',\n",
       " 'gin jocky',\n",
       " 'girl on top',\n",
       " 'go-kun',\n",
       " 'goatcx',\n",
       " 'goatse',\n",
       " 'god damn',\n",
       " 'god damned',\n",
       " 'god-damn',\n",
       " 'god-damned',\n",
       " 'goddamn',\n",
       " 'goddamned',\n",
       " 'gokkun',\n",
       " 'golden shower',\n",
       " 'golden showers',\n",
       " 'golliwog',\n",
       " 'gollywog',\n",
       " 'gook',\n",
       " 'gook-eye',\n",
       " 'gookie',\n",
       " 'gooks',\n",
       " 'gooky',\n",
       " 'goregasm',\n",
       " 'gray queen',\n",
       " 'greaseball',\n",
       " 'grey queen',\n",
       " 'grope',\n",
       " 'group sex',\n",
       " 'gym bunny',\n",
       " 'gymbunny',\n",
       " 'hadji',\n",
       " 'haji',\n",
       " 'hajji',\n",
       " 'hand job',\n",
       " 'hand-job',\n",
       " 'handjob',\n",
       " 'heimie',\n",
       " 'hell',\n",
       " 'hermie',\n",
       " 'hickory switch',\n",
       " 'hippophile',\n",
       " 'hippophilia',\n",
       " 'homoerotic',\n",
       " 'honkey',\n",
       " 'honkeys',\n",
       " 'honkies',\n",
       " 'honky',\n",
       " 'horny',\n",
       " 'horse shit',\n",
       " 'horse-shit',\n",
       " 'horseshit',\n",
       " 'hot carl',\n",
       " 'hot richard',\n",
       " 'huge cock',\n",
       " 'humping',\n",
       " 'hymie',\n",
       " 'impact play',\n",
       " 'impact-play',\n",
       " 'incest',\n",
       " 'intercourse',\n",
       " 'jack off',\n",
       " 'jack-off',\n",
       " 'jail bait',\n",
       " 'jailbait',\n",
       " 'jap',\n",
       " 'jelly donut',\n",
       " 'jerk mate',\n",
       " 'jerk off',\n",
       " 'jerk-off',\n",
       " 'jerkmate',\n",
       " 'jesus',\n",
       " 'jesus christ',\n",
       " 'jigaboo',\n",
       " 'jiggerboo',\n",
       " 'jizz',\n",
       " 'juggs',\n",
       " 'jungle bunny',\n",
       " 'junglebunny',\n",
       " 'kennebunkport surprise',\n",
       " 'kentucky klondike',\n",
       " 'kentucky tractor puller',\n",
       " 'kike',\n",
       " 'kinbaku',\n",
       " 'kitty puncher',\n",
       " 'kitty-puncher',\n",
       " 'kittypuncher',\n",
       " 'knobbing',\n",
       " 'kraut',\n",
       " 'krauts',\n",
       " 'kunt',\n",
       " 'kunts',\n",
       " 'kynophile',\n",
       " 'kynophilia',\n",
       " 'lady boy',\n",
       " 'lady-boy',\n",
       " 'ladyboy',\n",
       " 'leather restraint',\n",
       " 'leather straight jacket',\n",
       " 'lemon party',\n",
       " 'lemonparty',\n",
       " 'leningrad steamer',\n",
       " 'lesbo',\n",
       " 'leso',\n",
       " 'lezzie',\n",
       " 'lezzies',\n",
       " 'light in the fedora',\n",
       " 'light in the loafers',\n",
       " 'light in the pants',\n",
       " 'limp wristed',\n",
       " 'limp-wristed',\n",
       " 'literotica',\n",
       " 'lovemaking',\n",
       " 'male squirting',\n",
       " 'male-squirting',\n",
       " 'massive cock',\n",
       " 'masterb8',\n",
       " 'masterbate',\n",
       " 'masturb8',\n",
       " 'masturbate',\n",
       " 'masturbating',\n",
       " 'masturbation',\n",
       " 'mayonnaise monkey',\n",
       " 'mayonnaise monkies',\n",
       " 'mdlb',\n",
       " 'meat masseuse',\n",
       " 'meat spin',\n",
       " 'meatspin',\n",
       " 'menage a trois',\n",
       " 'menage-a-trois',\n",
       " 'menages a trois',\n",
       " 'menages-a-trois',\n",
       " 'menophile',\n",
       " 'menophilia',\n",
       " 'mexican pancake',\n",
       " 'milwaukee blizzard',\n",
       " 'missionary position',\n",
       " 'mississippi birdbath',\n",
       " 'mound of venus',\n",
       " 'mr hands',\n",
       " 'mr. hands',\n",
       " 'mrhands',\n",
       " 'muff diver',\n",
       " 'muff diver',\n",
       " 'muff diving',\n",
       " 'muff-diver',\n",
       " 'muffdiver',\n",
       " 'muffdiver',\n",
       " 'muffdiving',\n",
       " 'muscle mary',\n",
       " 'mvtube',\n",
       " 'nambla',\n",
       " 'necrophile',\n",
       " 'necrophilia',\n",
       " 'negro',\n",
       " 'neo nazi',\n",
       " 'neo-nazi',\n",
       " 'neonazi',\n",
       " 'nig nog',\n",
       " 'nigerian hurricane',\n",
       " 'nigga',\n",
       " 'nigger',\n",
       " 'niggs',\n",
       " 'nignog',\n",
       " 'nimpho',\n",
       " 'nimphomania',\n",
       " 'nimphomaniac',\n",
       " 'nipple',\n",
       " 'nipple clamp',\n",
       " 'nipple clamps',\n",
       " 'nipples',\n",
       " 'nude',\n",
       " 'nudity',\n",
       " 'nutten',\n",
       " 'nympho',\n",
       " 'nymphomania',\n",
       " 'nymphomaniac',\n",
       " 'octopussy',\n",
       " 'oklahomo',\n",
       " 'omorashi',\n",
       " 'one cup two girls',\n",
       " 'one jar one man',\n",
       " 'one man one jar',\n",
       " 'only fans',\n",
       " 'onlyfans',\n",
       " 'orgasm',\n",
       " 'orgasmic',\n",
       " 'orgasms',\n",
       " 'paedo bear',\n",
       " 'paedobear',\n",
       " 'paedophile',\n",
       " 'paedophilia',\n",
       " 'pain slut',\n",
       " 'painslut',\n",
       " 'paki',\n",
       " 'panamanian petting zoo',\n",
       " 'pansy',\n",
       " 'panties',\n",
       " 'parthenophile',\n",
       " 'parthenophilia',\n",
       " 'pedo bear',\n",
       " 'pedobear',\n",
       " 'pedophile',\n",
       " 'pedophilia',\n",
       " 'pegging',\n",
       " 'penis',\n",
       " 'peter puffer',\n",
       " 'peter-puffer',\n",
       " 'peterpuffer',\n",
       " 'petrol sniffer',\n",
       " 'petrol-sniffer',\n",
       " 'petrolsniffer',\n",
       " 'phagophile',\n",
       " 'phagophilia',\n",
       " 'piece of shit',\n",
       " 'pieces of shit',\n",
       " 'pikey',\n",
       " 'pikeys',\n",
       " 'piss off',\n",
       " 'piss pig',\n",
       " 'piss pig',\n",
       " 'pissed off',\n",
       " 'pissing',\n",
       " 'pisspig',\n",
       " 'pisspig',\n",
       " 'playboy',\n",
       " 'pleasure chest',\n",
       " 'pnigerophile',\n",
       " 'pnigerophilia',\n",
       " 'pnigophile',\n",
       " 'pnigophilia',\n",
       " 'poinephile',\n",
       " 'poinephilia',\n",
       " 'pony boy',\n",
       " 'pony girl',\n",
       " 'pony-boy',\n",
       " 'pony-girl',\n",
       " 'pony-play',\n",
       " 'ponyboy',\n",
       " 'ponygirl',\n",
       " 'ponyplay',\n",
       " 'poof',\n",
       " 'poon',\n",
       " 'poontang',\n",
       " 'poop chute',\n",
       " 'poopchute',\n",
       " 'porn',\n",
       " 'porn hub',\n",
       " 'pornhub',\n",
       " 'porno',\n",
       " 'pornographic',\n",
       " 'pornography',\n",
       " 'pornos',\n",
       " 'potato queen',\n",
       " 'prince albert piercing',\n",
       " 'proctophile',\n",
       " 'proctophilia',\n",
       " 'pubes',\n",
       " 'punani',\n",
       " 'punany',\n",
       " 'pussy',\n",
       " 'pussy puncher',\n",
       " 'pussy-puncher',\n",
       " 'pussypuncher',\n",
       " 'queaf',\n",
       " 'queef',\n",
       " 'quim',\n",
       " 'rag head',\n",
       " 'rag heads',\n",
       " 'raghead',\n",
       " 'ragheads',\n",
       " 'raging boner',\n",
       " 'ramen yarmulke',\n",
       " 'rape',\n",
       " 'raping',\n",
       " 'rapist',\n",
       " 'rectum',\n",
       " 'retard',\n",
       " 'retarded',\n",
       " 'reverse cowgirl',\n",
       " 'rhabdophile',\n",
       " 'rhabdophilia',\n",
       " 'rhypophile',\n",
       " 'rhypophilia',\n",
       " 'rice queen',\n",
       " 'rimjob',\n",
       " 'rimming',\n",
       " 'ring raider',\n",
       " 'ringraider',\n",
       " 'rusty trombone',\n",
       " 'sand nigger',\n",
       " 'sand-nigger',\n",
       " 'sandnigger',\n",
       " 'santorum',\n",
       " 'scatophile',\n",
       " 'scatophilia',\n",
       " 'schlong',\n",
       " 'scissoring',\n",
       " 'semen',\n",
       " 'seplophile',\n",
       " 'seplophilia',\n",
       " 'sex',\n",
       " 'shaved beaver',\n",
       " 'shaved pussy',\n",
       " 'she male',\n",
       " 'she-male',\n",
       " 'sheep shagger',\n",
       " 'sheepshagger',\n",
       " 'shemale',\n",
       " 'shibari',\n",
       " 'shit',\n",
       " 'shit head',\n",
       " 'shithead',\n",
       " 'shitty',\n",
       " 'shlong',\n",
       " 'shota',\n",
       " 'shrimping',\n",
       " 'sissy',\n",
       " 'skeet',\n",
       " 'skittle harvest',\n",
       " 'skittles harvest',\n",
       " 'slant eye',\n",
       " 'slant-eye',\n",
       " 'slanteye',\n",
       " 'snatch',\n",
       " 'snowballing',\n",
       " 'sod off',\n",
       " 'sodding',\n",
       " 'sodomise',\n",
       " 'sodomist',\n",
       " 'sodomize',\n",
       " 'sodomy',\n",
       " 'spastic',\n",
       " 'spearchucker',\n",
       " 'spic',\n",
       " 'spick',\n",
       " 'spicks',\n",
       " 'spics',\n",
       " 'spicy gringo',\n",
       " 'splooge',\n",
       " 'splooge moose',\n",
       " 'spooge',\n",
       " 'spunk',\n",
       " 'strap on',\n",
       " 'strap-on',\n",
       " 'strap-on',\n",
       " 'strapon',\n",
       " 'strappado',\n",
       " 'suastika',\n",
       " 'svastika',\n",
       " 'swamp guinea',\n",
       " 'swamp-guinea',\n",
       " 'swastika',\n",
       " 'switch hitter',\n",
       " 't-girl',\n",
       " 'taphephile',\n",
       " 'taphephilia',\n",
       " 'tea bagged',\n",
       " 'tea bagging',\n",
       " 'tea-bagged',\n",
       " 'tea-bagging',\n",
       " 'tgirl',\n",
       " 'thanatophile',\n",
       " 'thanatophilia',\n",
       " 'threesome',\n",
       " 'throating',\n",
       " 'throbbing boner',\n",
       " 'throbbing cock',\n",
       " 'thumbzilla',\n",
       " 'timber nigger',\n",
       " 'timber-nigger',\n",
       " 'timbernigger',\n",
       " 'tits',\n",
       " 'titties',\n",
       " 'titty',\n",
       " 'topless',\n",
       " 'tosser',\n",
       " 'towel head',\n",
       " 'towel-head',\n",
       " 'towelhead',\n",
       " 'trannie',\n",
       " 'tranny',\n",
       " 'transbian',\n",
       " 'traumatophile',\n",
       " 'traumatophilia',\n",
       " 'tribadism',\n",
       " 'tribbing',\n",
       " 'tub girl',\n",
       " 'tubgirl',\n",
       " 'twat',\n",
       " 'twink',\n",
       " 'two girls one cup',\n",
       " 'urethra play',\n",
       " 'urophile',\n",
       " 'urophilia',\n",
       " 'vagina',\n",
       " 'venus mound',\n",
       " 'viagra',\n",
       " 'vibrator',\n",
       " 'violet wand',\n",
       " 'vorarephile',\n",
       " 'vorarephilia',\n",
       " 'voyeurweb',\n",
       " 'wagon burner',\n",
       " 'wagon-burner',\n",
       " 'wank',\n",
       " 'wanker',\n",
       " 'wax play',\n",
       " 'wax-play',\n",
       " 'wet back',\n",
       " 'wet dream',\n",
       " 'wet-back',\n",
       " 'wetback',\n",
       " 'whigger',\n",
       " 'white power',\n",
       " 'white-power',\n",
       " 'whitepower',\n",
       " 'whore',\n",
       " 'wigga',\n",
       " 'wigger',\n",
       " 'wiitwd',\n",
       " 'wog',\n",
       " 'wogs',\n",
       " 'wolfbagging',\n",
       " 'worldsex',\n",
       " 'wrapping men',\n",
       " 'wrinkled starfish',\n",
       " 'xhamster',\n",
       " 'xnxx',\n",
       " 'xtube',\n",
       " 'xvideos',\n",
       " 'xxx',\n",
       " 'xyrophile',\n",
       " 'xyrophilia',\n",
       " 'yellow shower',\n",
       " 'yellow showers',\n",
       " 'zipper head',\n",
       " 'zipper-head',\n",
       " 'zipperhead',\n",
       " 'zippo cat',\n",
       " 'zippo-cat',\n",
       " 'zippocat',\n",
       " 'zoophile',\n",
       " 'zoophilia']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profanities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4bab9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to check for profanities\n",
    "def contains_profanity(text):\n",
    "    for profanity in profanities:\n",
    "        if profanity in text:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8763332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'Contains_Profanity' to mark records with profanities\n",
    "df['Contains_Profanity'] = df['Text'].apply(contains_profanity)\n",
    "\n",
    "# Filter the DataFrame to keep only clean records\n",
    "clean_df = df[df['Contains_Profanity'] == False]\n",
    "\n",
    "# Removed records will be stored in a DataFrame 'removed_records' by taking the complement of 'clean_df'.\n",
    "removed_records = df[df['Contains_Profanity'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd6a47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Contains_Profanity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i do not feel reassured anxiety is on each side</td>\n",
       "      <td>joy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>i didnt really feel that embarrassed</td>\n",
       "      <td>sadness</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>i can t imagine a real life scenario where i w...</td>\n",
       "      <td>joy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>i feel a bit rude writing to an elderly gentle...</td>\n",
       "      <td>anger</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17969</th>\n",
       "      <td>im feeling so damn gloomy too</td>\n",
       "      <td>sadness</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17979</th>\n",
       "      <td>i dont want to always be judgmental of particu...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17984</th>\n",
       "      <td>im thinking well i could be a bit smaller but ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17993</th>\n",
       "      <td>i feel tortured delilahlwl am considering i ha...</td>\n",
       "      <td>anger</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17999</th>\n",
       "      <td>im feeling more comfortable with derby i feel ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1630 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text Classification  \\\n",
       "1      i can go from feeling so hopeless to so damned...        sadness   \n",
       "15       i do not feel reassured anxiety is on each side            joy   \n",
       "16                  i didnt really feel that embarrassed        sadness   \n",
       "34     i can t imagine a real life scenario where i w...            joy   \n",
       "38     i feel a bit rude writing to an elderly gentle...          anger   \n",
       "...                                                  ...            ...   \n",
       "17969                      im feeling so damn gloomy too        sadness   \n",
       "17979  i dont want to always be judgmental of particu...        sadness   \n",
       "17984  im thinking well i could be a bit smaller but ...        sadness   \n",
       "17993  i feel tortured delilahlwl am considering i ha...          anger   \n",
       "17999  im feeling more comfortable with derby i feel ...            joy   \n",
       "\n",
       "       Contains_Profanity  \n",
       "1                    True  \n",
       "15                   True  \n",
       "16                   True  \n",
       "34                   True  \n",
       "38                   True  \n",
       "...                   ...  \n",
       "17969                True  \n",
       "17979                True  \n",
       "17984                True  \n",
       "17993                True  \n",
       "17999                True  \n",
       "\n",
       "[1630 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removed_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1f22262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1630, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removed_records.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf200941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8f6662f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16328, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e671f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'Contains_Profanity' column from the clean_df\n",
    "clean_df = clean_df.drop('Contains_Profanity', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bce70439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16328, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "746280a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy         5521\n",
       "sadness     4758\n",
       "anger       2190\n",
       "fear        2007\n",
       "love        1249\n",
       "surprise     603\n",
       "Name: Classification, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['Classification'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c8ea5f",
   "metadata": {},
   "source": [
    " <h3>NLP pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9f64245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0de52d7",
   "metadata": {},
   "source": [
    "Punkt Sentence Tokenizer.\n",
    "\n",
    "punkt tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model \n",
    "for abbreviation words, collocations, and words that start sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6671fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961f5b8",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "1. lower case\n",
    "2. tokenization\n",
    "3. removing stopword and punctuation\n",
    "4. stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ec89718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0b10f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing english stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85a5f6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e5ac117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing nltk.stem to stemming the words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "257897a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"playing\")  #testing if it is working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39311140",
   "metadata": {},
   "source": [
    "<b>Function to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b91b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_text(Comment):\n",
    "    # Convert text to lowercase\n",
    "    Comment = Comment.lower()\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = nltk.word_tokenize(Comment)\n",
    "\n",
    "    # Initialize the Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Removing English stopwords and applying stemming while ignoring special symbols\n",
    "    filtered_words = [stemmer.stem(word) for word in words if word not in stopwords.words('english') and word.isalnum()]\n",
    "\n",
    "    # Join the filtered words back into a single string\n",
    "    transformed_text = ' '.join(filtered_words)\n",
    "\n",
    "    return transformed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4980c7dd",
   "metadata": {},
   "source": [
    "<h3>adding a new preprocessed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c28d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[\"final_data\"]=clean_df[\"Text\"].apply(transformed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9422e384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Classification</th>\n",
       "      <th>final_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "      <td>didnt feel humili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "      <td>im grab minut post feel greedi wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "      <td>ever feel nostalg fireplac know still properti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>feel grouchi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ive been feeling a little burdened lately wasn...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>ive feel littl burden late wasnt sure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17995</th>\n",
       "      <td>i can feel its suffering</td>\n",
       "      <td>sadness</td>\n",
       "      <td>feel suffer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17996</th>\n",
       "      <td>i just keep feeling like someone is being unki...</td>\n",
       "      <td>anger</td>\n",
       "      <td>keep feel like someon unkind wrong think get b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17997</th>\n",
       "      <td>im feeling a little cranky negative after this...</td>\n",
       "      <td>anger</td>\n",
       "      <td>im feel littl cranki neg doctor appoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17998</th>\n",
       "      <td>i feel that i am useful to my people and that ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>feel use peopl give great feel achiev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18000</th>\n",
       "      <td>i feel all weird when i have to meet w people ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>feel weird meet w peopl text like dont talk fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16328 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text Classification  \\\n",
       "0                                i didnt feel humiliated        sadness   \n",
       "2       im grabbing a minute to post i feel greedy wrong          anger   \n",
       "3      i am ever feeling nostalgic about the fireplac...           love   \n",
       "4                                   i am feeling grouchy          anger   \n",
       "5      ive been feeling a little burdened lately wasn...        sadness   \n",
       "...                                                  ...            ...   \n",
       "17995                           i can feel its suffering        sadness   \n",
       "17996  i just keep feeling like someone is being unki...          anger   \n",
       "17997  im feeling a little cranky negative after this...          anger   \n",
       "17998  i feel that i am useful to my people and that ...            joy   \n",
       "18000  i feel all weird when i have to meet w people ...           fear   \n",
       "\n",
       "                                              final_data  \n",
       "0                                      didnt feel humili  \n",
       "2                   im grab minut post feel greedi wrong  \n",
       "3         ever feel nostalg fireplac know still properti  \n",
       "4                                           feel grouchi  \n",
       "5                  ive feel littl burden late wasnt sure  \n",
       "...                                                  ...  \n",
       "17995                                        feel suffer  \n",
       "17996  keep feel like someon unkind wrong think get b...  \n",
       "17997            im feel littl cranki neg doctor appoint  \n",
       "17998              feel use peopl give great feel achiev  \n",
       "18000  feel weird meet w peopl text like dont talk fa...  \n",
       "\n",
       "[16328 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec76cd1",
   "metadata": {},
   "source": [
    "<h3> Label encoding y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3e46a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding target column using LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()\n",
    "clean_df[\"Classification\"]=encoder.fit_transform(clean_df[\"Classification\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ecb938a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Classification</th>\n",
       "      <th>final_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>4</td>\n",
       "      <td>didnt feel humili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>0</td>\n",
       "      <td>im grab minut post feel greedi wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>3</td>\n",
       "      <td>ever feel nostalg fireplac know still properti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>0</td>\n",
       "      <td>feel grouchi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ive been feeling a little burdened lately wasn...</td>\n",
       "      <td>4</td>\n",
       "      <td>ive feel littl burden late wasnt sure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17995</th>\n",
       "      <td>i can feel its suffering</td>\n",
       "      <td>4</td>\n",
       "      <td>feel suffer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17996</th>\n",
       "      <td>i just keep feeling like someone is being unki...</td>\n",
       "      <td>0</td>\n",
       "      <td>keep feel like someon unkind wrong think get b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17997</th>\n",
       "      <td>im feeling a little cranky negative after this...</td>\n",
       "      <td>0</td>\n",
       "      <td>im feel littl cranki neg doctor appoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17998</th>\n",
       "      <td>i feel that i am useful to my people and that ...</td>\n",
       "      <td>2</td>\n",
       "      <td>feel use peopl give great feel achiev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18000</th>\n",
       "      <td>i feel all weird when i have to meet w people ...</td>\n",
       "      <td>1</td>\n",
       "      <td>feel weird meet w peopl text like dont talk fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16328 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Classification  \\\n",
       "0                                i didnt feel humiliated               4   \n",
       "2       im grabbing a minute to post i feel greedy wrong               0   \n",
       "3      i am ever feeling nostalgic about the fireplac...               3   \n",
       "4                                   i am feeling grouchy               0   \n",
       "5      ive been feeling a little burdened lately wasn...               4   \n",
       "...                                                  ...             ...   \n",
       "17995                           i can feel its suffering               4   \n",
       "17996  i just keep feeling like someone is being unki...               0   \n",
       "17997  im feeling a little cranky negative after this...               0   \n",
       "17998  i feel that i am useful to my people and that ...               2   \n",
       "18000  i feel all weird when i have to meet w people ...               1   \n",
       "\n",
       "                                              final_data  \n",
       "0                                      didnt feel humili  \n",
       "2                   im grab minut post feel greedi wrong  \n",
       "3         ever feel nostalg fireplac know still properti  \n",
       "4                                           feel grouchi  \n",
       "5                  ive feel littl burden late wasnt sure  \n",
       "...                                                  ...  \n",
       "17995                                        feel suffer  \n",
       "17996  keep feel like someon unkind wrong think get b...  \n",
       "17997            im feel littl cranki neg doctor appoint  \n",
       "17998              feel use peopl give great feel achiev  \n",
       "18000  feel weird meet w peopl text like dont talk fa...  \n",
       "\n",
       "[16328 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eff6ac1",
   "metadata": {},
   "source": [
    "<h3>Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e33a802",
   "metadata": {},
   "source": [
    "Using count vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a96e66ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer #importing countvector\n",
    "cvector=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da0da528",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=cvector.fit_transform(clean_df[\"final_data\"]).toarray() #applying vectorization to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b486c7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbaf8e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=clean_df[\"Classification\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f755be54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 3, ..., 0, 2, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd69241",
   "metadata": {},
   "source": [
    "<h3>seperating training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35b89070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing traintest split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=3) # 20% data will used for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8bad10",
   "metadata": {},
   "source": [
    "<h3>model initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0401744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score #imprting classification metrics\n",
    "from sklearn.linear_model import LogisticRegression #importing logistic regression\n",
    "from sklearn.svm import SVC #importing Support vector classifier\n",
    "from sklearn.tree import DecisionTreeClassifier #importing Decision tree classifier\n",
    "from sklearn.ensemble import RandomForestClassifier #importing Random forest\n",
    "from sklearn.naive_bayes import MultinomialNB   #importing multinomial naive bayes\n",
    "from sklearn.neighbors import KNeighborsClassifier  #importing KNN\n",
    "from sklearn.ensemble import AdaBoostClassifier #importing Adaboost classifier\n",
    "from sklearn.ensemble import BaggingClassifier  #importing bagging classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier #importing GB classifier\n",
    "from xgboost import XGBClassifier   #importing XGB classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd01e0",
   "metadata": {},
   "source": [
    "<h3>Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d199b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IHSAN B P\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on train data is  0.9735875057418466\n",
      "precision score on train data is  0.9705224054940526\n",
      "accuracy score on test data is  0.8542559706062461\n",
      "precision score on test data is  0.823737194655629\n"
     ]
    }
   ],
   "source": [
    "log_reg=LogisticRegression()    #assigning model\n",
    "log_reg.fit(x_train,y_train)    #model training\n",
    "y_log_pred=log_reg.predict(x_test)  #predicting on test data\n",
    "yt_log_pred=log_reg.predict(x_train)    #predicting on train data\n",
    "log_reg_acc=accuracy_score(y_test,y_log_pred)   #accuracy on test data\n",
    "log_reg_prec=precision_score(y_test,y_log_pred,average='macro') #precision on test data\n",
    "\n",
    "tr_log_reg_acc=accuracy_score(y_train,yt_log_pred)  #accuracy on test data\n",
    "tr_log_reg_prec=precision_score(y_train,yt_log_pred,average='macro') #precision on test data\n",
    "\n",
    "#printing accuracy and precision\n",
    "print(\"accuracy score on train data is \",tr_log_reg_acc)\n",
    "print(\"precision score on train data is \",tr_log_reg_prec)\n",
    "print(\"accuracy score on test data is \",log_reg_acc)\n",
    "print(\"precision score on test data is \",log_reg_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1620fb6f",
   "metadata": {},
   "source": [
    "<h3>Support vector classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45e395bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sv=SVC()   #assigning model\n",
    "# sv.fit(x_train,y_train) #model training\n",
    "# sv_pred=sv.predict(x_test)  #predicting on test data\n",
    "# svt_pred=sv.predict(x_train)    #predicting on train data\n",
    "# sv_acc=accuracy_score(y_test,sv_pred)   #accuracy score on test data\n",
    "# sv_prec=precision_score(y_test,sv_pred,average='macro') #precision on test data\n",
    "# svt_acc=accuracy_score(y_train,svt_pred)    #accuracy on train data\n",
    "# svt_prec=precision_score(y_train,svt_pred,average='macro')  #precision on train data\n",
    "\n",
    "# #printing accuracy and precision\n",
    "# print(\"accuracy score on train datais \",svt_acc)\n",
    "# print(\"precision score on train data is \",svt_prec)\n",
    "# print(\"accuracy score on test data is \",sv_acc)\n",
    "# print(\"precision score on test data is \",sv_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f380fa92",
   "metadata": {},
   "source": [
    "<h3>Decision tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59a84437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on train data is  0.9995406522737712\n",
      "precision score on train data is  0.9993997479735985\n",
      "accuracy score on test data is  0.8034292712798531\n",
      "precision score on test data is  0.7462801306745096\n"
     ]
    }
   ],
   "source": [
    "dec_tree=DecisionTreeClassifier()   #assigning model\n",
    "dec_tree.fit(x_train,y_train)   #model training\n",
    "dec_tree_pred=dec_tree.predict(x_test)  #prediction on test data\n",
    "dec_tree_tr_pred=dec_tree.predict(x_train)  #prediction on train data\n",
    "\n",
    "dec_tree_acc=accuracy_score(y_test,dec_tree_pred)   #accuracy of test data\n",
    "dec_tree_prec=precision_score(y_test,dec_tree_pred,average='macro') #precision of test data\n",
    "dec_tree_tr_acc=accuracy_score(y_train,dec_tree_tr_pred)    #accuracy of train data\n",
    "dec_tree_tr_prec=precision_score(y_train,dec_tree_tr_pred,average='macro')  #precision on train data\n",
    "#printing accuracy and precision\n",
    "print(\"accuracy score on train data is \",dec_tree_tr_acc)\n",
    "print(\"precision score on train data is \",dec_tree_tr_prec)\n",
    "print(\"accuracy score on test data is \",dec_tree_acc)\n",
    "print(\"precision score on test data is \",dec_tree_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dae3e4",
   "metadata": {},
   "source": [
    "<h3>Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be9ec908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on train data  is  0.9995406522737712\n",
      "precision score on train data  is  0.9994593191131237\n",
      "accuracy score on test data is  0.852112676056338\n",
      "precision score on test data  is  0.7986990672134642\n"
     ]
    }
   ],
   "source": [
    "rfcl_model=RandomForestClassifier() #assigning model\n",
    "rfcl_model.fit(x_train,y_train) #model training\n",
    "rfcl_pred_model=rfcl_model.predict(x_test)  #prediction on test data\n",
    "rfcl_tr_pred_model=rfcl_model.predict(x_train)  #prediction on train data\n",
    "rfcl_acc_model=accuracy_score(y_test,rfcl_pred_model)   #accuracy on test data\n",
    "rfcl_prec_model=precision_score(y_test,rfcl_pred_model,average='macro') #precision on test data\n",
    "rfcl_tr_acc_model=accuracy_score(y_train,rfcl_tr_pred_model)    #accuracy on train data\n",
    "rfcl_tr_prec_model=precision_score(y_train,rfcl_tr_pred_model,average='macro')  #precision on train data\n",
    "#printing accuracy and precision\n",
    "\n",
    "print(\"accuracy score on train data  is \",rfcl_tr_acc_model)\n",
    "print(\"precision score on train data  is \",rfcl_tr_prec_model)\n",
    "print(\"accuracy score on test data is \",rfcl_acc_model)\n",
    "print(\"precision score on test data  is \",rfcl_prec_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c83f71e",
   "metadata": {},
   "source": [
    "<h3>Naive Bayes classifier</h3>\n",
    "MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "225c5802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on train data is  0.8714591946103201\n",
      "precision score on train data is  0.9240836037146307\n",
      "accuracy score on test data is  0.7676056338028169\n",
      "precision score on test data is  0.7953391178059007\n"
     ]
    }
   ],
   "source": [
    "mnb=MultinomialNB() #assigning model\n",
    "mnb.fit(x_train,y_train)    #model training\n",
    "mnb_pred=mnb.predict(x_test)    #prediction on test data\n",
    "mnb_tr_pred=mnb.predict(x_train)    #prediction on train data\n",
    "mnb_acc=accuracy_score(y_test,mnb_pred) #accuracy on test data\n",
    "mnb_prec=precision_score(y_test,mnb_pred,average='macro')   #precision on test data\n",
    "mnb_tr_acc=accuracy_score(y_train,mnb_tr_pred)  #accuracy on train data\n",
    "mnb_tr_prec=precision_score(y_train,mnb_tr_pred,average='macro')    #precision on train data\n",
    "#printing accuracy and precision\n",
    "print(\"accuracy score on train data is \",mnb_tr_acc)\n",
    "print(\"precision score on train data is \",mnb_tr_prec)\n",
    "print(\"accuracy score on test data is \",mnb_acc)\n",
    "print(\"precision score on test data is \",mnb_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0178c314",
   "metadata": {},
   "source": [
    "<h3>XGboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "181b3d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on train data is  0.929643239932629\n",
      "precision score on train data is  0.9081245542656996\n",
      "accuracy score on test data is  0.865278628291488\n",
      "precision score on test data is  0.8234217867225458\n"
     ]
    }
   ],
   "source": [
    "xgb=XGBClassifier() #assigning model\n",
    "xgb.fit(x_train,y_train)    #model training\n",
    "xgb_pred=xgb.predict(x_test)    #prediction on test data\n",
    "xgb_tr_pred=xgb.predict(x_train)    #prediction on train data\n",
    "xgb_acc=accuracy_score(y_test,xgb_pred) #accuracy on test data\n",
    "xgb_prec=precision_score(y_test,xgb_pred,average='macro')   #precision on test data\n",
    "xgb_tr_acc=accuracy_score(y_train,xgb_tr_pred)  #accuracy on train data\n",
    "xgb_tr_prec=precision_score(y_train,xgb_tr_pred,average='macro')    #precision on train data\n",
    "#printing accuracy and precision\n",
    "print(\"accuracy score on train data is \",xgb_tr_acc)\n",
    "print(\"precision score on train data is \",xgb_tr_prec)\n",
    "print(\"accuracy score on test data is \",xgb_acc)\n",
    "print(\"precision score on test data is \",xgb_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66ffaf",
   "metadata": {},
   "source": [
    "<h3>Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7389e4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on train data is  0.3716888684734344\n",
      "precision score on train data is  0.5622119830342891\n",
      "accuracy score on test data is  0.37630128597672996\n",
      "precision score on test data is  0.5716944614507886\n"
     ]
    }
   ],
   "source": [
    "adb=AdaBoostClassifier()    #assigning model\n",
    "adb.fit(x_train,y_train)    #model training\n",
    "adb_pred=adb.predict(x_test)    #prediction on test data\n",
    "adb_tr_pred=adb.predict(x_train)    #prediction on train data\n",
    "adb_acc=accuracy_score(y_test,adb_pred) #accuracy on test data\n",
    "adb_prec=precision_score(y_test,adb_pred,average='macro')   #precision on test data\n",
    "adb_tr_acc=accuracy_score(y_train,adb_tr_pred)  #accuracy on train data\n",
    "adb_tr_prec=precision_score(y_train,adb_tr_pred,average='macro')    #precision on train data\n",
    "#printing accuracy and precision\n",
    "print(\"accuracy score on train data is \",adb_tr_acc)\n",
    "print(\"precision score on train data is \",adb_tr_prec)\n",
    "print(\"accuracy score on test data is \",adb_acc)\n",
    "print(\"precision score on test data is \",adb_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ef1a6b",
   "metadata": {},
   "source": [
    "<h3>Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9672d341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on train data is  0.8480324605726535\n",
      "precision score on train data is  0.8685267827772306\n",
      "accuracy score on test data is  0.8000612369871403\n",
      "precision score on test data is  0.8107979095453418\n"
     ]
    }
   ],
   "source": [
    "gbc=GradientBoostingClassifier()    #assigning model\n",
    "gbc.fit(x_train,y_train)    #model training\n",
    "gbc_pred=gbc.predict(x_test)    #prediction on test data\n",
    "gbc_tr_pred=gbc.predict(x_train)    #prediction on train data\n",
    "gbc_acc=accuracy_score(y_test,gbc_pred) #accuracy on test data\n",
    "gbc_prec=precision_score(y_test,gbc_pred,average='macro')   #precision on test data\n",
    "gbc_tr_acc=accuracy_score(y_train,gbc_tr_pred)  #accuracy on train data\n",
    "gbc_tr_prec=precision_score(y_train,gbc_tr_pred,average='macro')    #precision on train data\n",
    "#printing accuracy and precision\n",
    "print(\"accuracy score on train data is \",gbc_tr_acc)\n",
    "print(\"precision score on train data is \",gbc_tr_prec)\n",
    "print(\"accuracy score on test data is \",gbc_acc)\n",
    "print(\"precision score on test data is \",gbc_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3081b8",
   "metadata": {},
   "source": [
    "<h3>Bagging Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7472ca65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on train data is  0.9826979023120502\n",
      "precision score on train data is  0.9783831201429503\n",
      "accuracy score on test data is  0.8230251071647275\n",
      "precision score on test data is  0.7656004325336107\n"
     ]
    }
   ],
   "source": [
    "bagc=BaggingClassifier()    #assigning model\n",
    "bagc.fit(x_train,y_train)   #model training\n",
    "bagc_pred=bagc.predict(x_test)  #prediction on test\n",
    "bagc_tr_pred=bagc.predict(x_train)  #prediction on train data\n",
    "bagc_acc=accuracy_score(y_test,bagc_pred)   #accuracy on test data\n",
    "bagc_prec=precision_score(y_test,bagc_pred,average='macro') #precision on test data\n",
    "bagc_tr_acc=accuracy_score(y_train,bagc_tr_pred)    #accuracy on train data\n",
    "bagc_tr_prec=precision_score(y_train,bagc_tr_pred,average='macro')  #precision on train data\n",
    "#printing accuracy and precision\n",
    "print(\"accuracy score on train data is \",bagc_tr_acc)\n",
    "print(\"precision score on train data is \",bagc_tr_prec)\n",
    "print(\"accuracy score on test data is \",bagc_acc)\n",
    "print(\"precision score on test data is \",bagc_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49258f34",
   "metadata": {},
   "source": [
    "<h3>KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb979afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on train data is  0.7720104118817945\n",
      "precision score on train data is  0.749694402060667\n",
      "accuracy score on test data is  0.612369871402327\n",
      "precision score on test data is  0.5493585433791226\n"
     ]
    }
   ],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=5) #assigning model\n",
    "knn.fit(x_train,y_train)    #training of model\n",
    "knn_pred=knn.predict(x_test)    #prediction on test data\n",
    "knn_tr_pred=knn.predict(x_train)    #prediction on train data\n",
    "knn_acc=accuracy_score(y_test,knn_pred) #accuracy score on test data\n",
    "knn_prec=precision_score(y_test,knn_pred,average='macro')   #precision on test data\n",
    "knn_tr_acc=accuracy_score(y_train,knn_tr_pred)  #accuracy on train data\n",
    "knn_tr_prec=precision_score(y_train,knn_tr_pred,average='macro')    #precision on train data\n",
    "#printing accuracy and precision\n",
    "print(\"accuracy score on train data is \",knn_tr_acc)\n",
    "print(\"precision score on train data is \",knn_tr_prec)\n",
    "print(\"accuracy score on test data is \",knn_acc)\n",
    "print(\"precision score on test data is \",knn_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e54d87",
   "metadata": {},
   "source": [
    "<h3>DataFrame to show performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10cd405b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>train_precision_score</th>\n",
       "      <th>test_precision_score</th>\n",
       "      <th>train_accuracy_score</th>\n",
       "      <th>test_accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>0.970522</td>\n",
       "      <td>0.823737</td>\n",
       "      <td>0.973588</td>\n",
       "      <td>0.854256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>0.746280</td>\n",
       "      <td>0.999541</td>\n",
       "      <td>0.803429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.999459</td>\n",
       "      <td>0.798699</td>\n",
       "      <td>0.999541</td>\n",
       "      <td>0.852113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>multinomial_NB</td>\n",
       "      <td>0.924084</td>\n",
       "      <td>0.795339</td>\n",
       "      <td>0.871459</td>\n",
       "      <td>0.767606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.908125</td>\n",
       "      <td>0.823422</td>\n",
       "      <td>0.929643</td>\n",
       "      <td>0.865279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.562212</td>\n",
       "      <td>0.571694</td>\n",
       "      <td>0.371689</td>\n",
       "      <td>0.376301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gradientboost</td>\n",
       "      <td>0.868527</td>\n",
       "      <td>0.810798</td>\n",
       "      <td>0.848032</td>\n",
       "      <td>0.800061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bagging</td>\n",
       "      <td>0.978383</td>\n",
       "      <td>0.765600</td>\n",
       "      <td>0.982698</td>\n",
       "      <td>0.823025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>knn</td>\n",
       "      <td>0.749694</td>\n",
       "      <td>0.549359</td>\n",
       "      <td>0.772010</td>\n",
       "      <td>0.612370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model_name  train_precision_score  test_precision_score  \\\n",
       "0  logistic_regression               0.970522              0.823737   \n",
       "1        decision_tree               0.999400              0.746280   \n",
       "2        random_forest               0.999459              0.798699   \n",
       "3       multinomial_NB               0.924084              0.795339   \n",
       "4              xgboost               0.908125              0.823422   \n",
       "5             adaboost               0.562212              0.571694   \n",
       "6        gradientboost               0.868527              0.810798   \n",
       "7              bagging               0.978383              0.765600   \n",
       "8                  knn               0.749694              0.549359   \n",
       "\n",
       "   train_accuracy_score  test_accuracy_score  \n",
       "0              0.973588             0.854256  \n",
       "1              0.999541             0.803429  \n",
       "2              0.999541             0.852113  \n",
       "3              0.871459             0.767606  \n",
       "4              0.929643             0.865279  \n",
       "5              0.371689             0.376301  \n",
       "6              0.848032             0.800061  \n",
       "7              0.982698             0.823025  \n",
       "8              0.772010             0.612370  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"model_name\":[\"logistic_regression\",\"decision_tree\",\"random_forest\",\"multinomial_NB\",\"xgboost\",\"adaboost\",\"gradientboost\",\"bagging\",\"knn\"],\n",
    "              \"train_precision_score\":[tr_log_reg_prec,dec_tree_tr_prec,rfcl_tr_prec_model,mnb_tr_prec,xgb_tr_prec,adb_tr_prec,gbc_tr_prec,bagc_tr_prec,knn_tr_prec],\n",
    "              \"test_precision_score\":[log_reg_prec,dec_tree_prec,rfcl_prec_model,mnb_prec,xgb_prec,adb_prec,gbc_prec,bagc_prec,knn_prec],\n",
    "              \"train_accuracy_score\":[tr_log_reg_acc,dec_tree_tr_acc,rfcl_tr_acc_model,mnb_tr_acc,xgb_tr_acc,adb_tr_acc,gbc_tr_acc,bagc_tr_acc,knn_tr_acc],\n",
    "              \"test_accuracy_score\":[log_reg_acc,dec_tree_acc,rfcl_acc_model,mnb_acc,xgb_acc,adb_acc,gbc_acc,bagc_acc,knn_acc]\n",
    "              })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38d544",
   "metadata": {},
   "source": [
    "<h4>Decision tree model performing well but it can lead overfitting in data, we can consider Random forest classifier as they are giving best result as precision and accuracy score is well balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b1a84a",
   "metadata": {},
   "source": [
    "<h3>Testing with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34a76ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion is love\n"
     ]
    }
   ],
   "source": [
    "#'rfcl_model' is random forrest model\n",
    "\n",
    "# user text\n",
    "user_text = \"i hope that the next quote will be able to let my special someone knows what im feeling insecure about and understand that no matter how much i trust\"\n",
    "# Transform the given text\n",
    "transformed_user_data = transformed_text(user_text)\n",
    "# Vectorize the transformed text\n",
    "text_vectorized = cvector.transform([transformed_user_data]).toarray()\n",
    "\n",
    "#predictions using the model\n",
    "prediction = rfcl_model.predict(text_vectorized)\n",
    "\n",
    "# Print the prediction\n",
    "if prediction==0:\n",
    "    print(\"emotion is anger\")\n",
    "elif prediction==1:\n",
    "    print(\"emotion is fear\")\n",
    "elif prediction==2:\n",
    "    print(\"emotion is joy\")\n",
    "elif prediction==3:\n",
    "    print(\"emotion is love\")\n",
    "elif prediction==4:\n",
    "    print(\"emotion is sadness\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaccb71",
   "metadata": {},
   "source": [
    "<h3>Prediction Consol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ddf72100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text (or 'exit' to quit): i love you\n",
      "Predicted emotion is: love\n",
      "\n",
      "Enter text (or 'exit' to quit): i hate you\n",
      "Predicted emotion is: anger\n",
      "\n",
      "Enter text (or 'exit' to quit): exit\n"
     ]
    }
   ],
   "source": [
    "# Function to get user input and make predictions\n",
    "def predict_emotion():\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_text = input(\"Enter text (or 'exit' to quit): \")\n",
    "\n",
    "        # Check for exit command\n",
    "        if user_text.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        # Transform the given text\n",
    "        transformed_user_data = transformed_text(user_text)\n",
    "\n",
    "        # Vectorize the transformed text\n",
    "        text_vectorized = cvector.transform([transformed_user_data]).toarray()\n",
    "\n",
    "        # Make predictions using the model\n",
    "        prediction = rfcl_model.predict(text_vectorized)\n",
    "\n",
    "        # Map the prediction to emotion\n",
    "        emotions = ['anger', 'fear', 'joy', 'love', 'sadness']\n",
    "        emotion = emotions[prediction[0]]\n",
    "\n",
    "        # Print the prediction\n",
    "        print(f\"Predicted emotion is: {emotion}\\n\")\n",
    "\n",
    "# Run the prediction function\n",
    "predict_emotion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03a6d45",
   "metadata": {},
   "source": [
    "<h3>Saving madel and Vectorizer in pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b1e7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "67fb11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6600e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: C:\\Users\\IHSAN B P\\text classification\\my_model.pkl\n",
      "vectorizer saved to: C:\\Users\\IHSAN B P\\text classification\\my_vectorizer\n"
     ]
    }
   ],
   "source": [
    "# Specify the filename for saving the model\n",
    "model_filename = 'my_model.pkl'\n",
    "vectorizer_name = 'my_vectorizer'\n",
    "# Construct the full file path\n",
    "model_path = os.path.join(current_directory, model_filename)\n",
    "vectorizer_path = os.path.join(current_directory, vectorizer_name)\n",
    "# Save the model to the specified path\n",
    "joblib.dump(rfcl_model, model_path)\n",
    "joblib.dump(rfcl_model, vectorizer_path)\n",
    "# Print the saved model path\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"vectorizer saved to: {vectorizer_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c6db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f59f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
